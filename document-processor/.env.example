# Это шаблон файла конфигурации.
# Скопируйте этот файл в .env и отредактируйте его, вписав ваши реальные данные.
# --------------------------------------------------------------------------

# --- Настройки базы данных PostgreSQL ---
# Адрес и порт сервера PostgreSQL
DB_HOST=192.168.1.148
DB_PORT=54321
# Имя базы данных, пользователь и пароль
DB_NAME=bot
DB_USER=botuser
DB_PASSWORD=botpass

# --- Настройки файлового хранилища MinIO ---
# Адрес и порт сервера MinIO
MINIO_ENDPOINT=192.168.1.148:9000
# Ключи доступа
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
# Имя бакета (должен быть создан заранее)
MINIO_BUCKET_NAME=kbase

# --- Настройки графовой базы данных Neo4j ---
# Включить/отключить интеграцию с Neo4j (true/false)
NEO4J_ENABLED=false
# Адрес и порт сервера Neo4j
NEO4J_URI=bolt://192.168.1.148:7687
# Пользователь и пароль
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4jadmin

# --- Настройки модели для эмбеддингов ---
# Имя модели из HuggingFace для создания векторных представлений
EMBEDDING_MODEL_NAME=BAAI/bge-m3

# --- Настройки LLM сервиса (Ollama, LM Studio, и т.д.) ---
# ВАЖНО: Укажите базовый URL вашего LLM-сервиса БЕЗ /api или /v1
# Например: http://localhost:11434 для Ollama или http://localhost:1234 для LM Studio
LLM_API_BASE=http://localhost:11434
# Имя модели, которая будет использоваться для обогащения данных
LLM_MODEL=gpt-oss:20b
# Таймаут для длинных запросов к LLM (в секундах)
LLM_TIMEOUT=300
# Таймаут для быстрых запросов health-check к LLM (в секундах)
LLM_HEALTH_TIMEOUT=10

# --- Настройки воркеров и обработки ---
# Уровень логирования (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO
# Интервал опроса базы данных на наличие новых задач (в секундах)
POLL_INTERVAL=10
# Количество потоков для обработки новых документов
UPLOAD_WORKER_COUNT=2
# Количество потоков для обогащения данных через LLM
ENRICHMENT_WORKER_COUNT=1
# Количество потоков для обработки задач на удаление
DELETION_WORKER_COUNT=1
# Размер батча (количество чанков) для одного цикла воркера обогащения
ENRICHMENT_BATCH_SIZE=10
# Размер батча для генерации эмбеддингов (подбирается под VRAM вашей GPU)
EMBEDDING_BATCH_SIZE=64
# Максимальный размер контекста в токенах для макро-чанков, отправляемых в LLM
LLM_CONTEXT_TOKEN_LIMIT=4000

# --- Настройки парсеров ---
# Включить/отключить распознавание текста на картинках (OCR)
OCR_ENABLED=true
# Языки для OCR (например, "rus+eng" для русского и английского)
OCR_LANG=rus+eng
# Количество строк из Excel, объединяемых в один чанк
EXCEL_ROW_BATCH_SIZE=10

# --- Настройки SmartChunker ---
# Целевой размер одного чанка в токенах
CHUNKER_CHUNK_TOKENS=500
# Размер перекрытия между чанками в токенах
CHUNKER_OVERLAP_TOKENS=80
# Максимальный размер секции/параграфа в токенах (если больше - будет нарезан)
CHUNKER_SECTION_LIMIT=2000
# Максимальный размер таблицы в токенах (если больше - будет нарезана с сохранением заголовка)
CHUNKER_TABLE_LIMIT=2000
# Максимальный размер списка в токенах (если больше - будет нарезан)
CHUNKER_LIST_LIMIT=1500